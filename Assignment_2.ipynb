{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT5135 Research Topics in AI\n",
    "\n",
    "## Assignment 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Student ID(s): 22229358, 22230186, 20230220\n",
    "* Student name(s): KOSTADIN GEORGIEV, YAMINI GIRKAR, SHUBHAM MANGLAM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, activations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_directory(dir, callback, label=None):\n",
    "    for dirname, _, filenames in os.walk(dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                id = filename[:-4]\n",
    "                pathname = os.path.join(dirname, filename)\n",
    "                im = Image.open(pathname)\n",
    "                imnp = np.array(im, dtype=float)\n",
    "\n",
    "                if len(imnp.shape) != 3:\n",
    "                    print(\"This is 1 channel, so we omit it\",\n",
    "                          imnp.shape, filename)\n",
    "                    continue\n",
    "\n",
    "                callback(id, imnp, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 1 channel, so we omit it (683, 1024) 4b9ef3ce2685ee32.jpg\n",
      "This is 1 channel, so we omit it (914, 1024) 5a71db307230880e.jpg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>channels</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01ad3ff1d94eb557</th>\n",
       "      <td>670</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>alpaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0346463867a297f4</th>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>alpaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>038fae9e70c4c3f1</th>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>alpaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>053608552d63f724</th>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>alpaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>053dab62fbb47736</th>\n",
       "      <td>682</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>alpaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb4d2e0c0252fdc2</th>\n",
       "      <td>682</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>not_alpaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb4f5968c7866a3e</th>\n",
       "      <td>699</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>not_alpaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f4495f8511553631</th>\n",
       "      <td>680</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>not_alpaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f8d3c3d8be68c4fd</th>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>not_alpaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f8e3dd41b584f645</th>\n",
       "      <td>768</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>not_alpaca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>325 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  height  width  channels       label\n",
       "01ad3ff1d94eb557     670   1024         3      alpaca\n",
       "0346463867a297f4     768   1024         3      alpaca\n",
       "038fae9e70c4c3f1     768   1024         3      alpaca\n",
       "053608552d63f724     768   1024         3      alpaca\n",
       "053dab62fbb47736     682   1024         3      alpaca\n",
       "...                  ...    ...       ...         ...\n",
       "eb4d2e0c0252fdc2     682   1024         3  not_alpaca\n",
       "eb4f5968c7866a3e     699   1024         3  not_alpaca\n",
       "f4495f8511553631     680   1024         3  not_alpaca\n",
       "f8d3c3d8be68c4fd     768   1024         3  not_alpaca\n",
       "f8e3dd41b584f645     768   1024         3  not_alpaca\n",
       "\n",
       "[325 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['height', 'width', 'channels', 'label'])\n",
    "\n",
    "def get_img_shape(id, imnp, label):\n",
    "    height, width, channel = imnp.shape\n",
    "    df.loc[id] = [height, width, channel, label]\n",
    "\n",
    "walk_directory('dataset/alpaca', get_img_shape, 'alpaca')\n",
    "walk_directory('dataset/not_alpaca', get_img_shape, 'not_alpaca')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images will be resized to 481 x 583\n"
     ]
    }
   ],
   "source": [
    "IMG_HEIGHT = df['height'].min()\n",
    "IMG_WIDTH = df['width'].min()\n",
    "\n",
    "print('Images will be resized to', IMG_HEIGHT, 'x', IMG_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(label):\n",
    "    source_dir = 'dataset/' + label\n",
    "    target_dir = 'dataset_resized/'+ label\n",
    "    subset = df.loc[df['label'] == label]\n",
    "\n",
    "    for id, image in subset.iterrows():\n",
    "        id = str(id)\n",
    "        filename = id + \".jpg\"\n",
    "        \n",
    "        source_path = os.path.join(source_dir, filename)\n",
    "        target_path = os.path.join(target_dir, filename)\n",
    "        \n",
    "        image = Image.open(source_path)\n",
    "        image = image.resize((IMG_WIDTH, IMG_HEIGHT), Image.NEAREST)\n",
    "        image.save(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_images('alpaca')\n",
    "resize_images('not_alpaca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(dir):\n",
    "    img_array = []\n",
    "    \n",
    "    walk_directory(dir, lambda id, imnp, label: img_array.append(imnp))\n",
    "\n",
    "    return np.array(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_alpaca = load_images('dataset_resized/alpaca')\n",
    "img_not_alpaca = load_images('dataset_resized/not_alpaca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpaca: (142, 481, 583, 3)\n",
      "not_alpaca: (183, 481, 583, 3)\n"
     ]
    }
   ],
   "source": [
    "print('alpaca:', img_alpaca.shape)\n",
    "print('not_alpaca:', img_not_alpaca.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Proposed Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Finish and verfiy implementation\n",
    "# Current implementation creates a regular Conv2D layer\n",
    "\n",
    "class ProposedLayer(layers.Layer):\n",
    "    def __init__(self, filters, receptive_field_size=(3, 3), activation=None, **kwargs):\n",
    "        super(ProposedLayer, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.receptive_field_size = receptive_field_size\n",
    "        self.activation = activations.get(activation)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        n_samples, height, width, channels = input_shape\n",
    "\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(height, width, channels, self.filters),\n",
    "                                      initializer='random_normal',\n",
    "                                      trainable=True)\n",
    "        self.bias = self.add_weight(name='bias',\n",
    "                                    shape=(self.filters,),\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        super(ProposedLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        conv = tf.nn.conv2d(inputs, self.kernel, strides=(1, 1), padding='SAME')\n",
    "        z = tf.nn.bias_add(conv, self.bias)\n",
    "        a = self.activation(z)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        rf = self.receptive_field_size\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[1] = (input_shape[1] - rf[0]) // 1 + 1\n",
    "        output_shape[2] = (input_shape[2] - rf[1]) // 1 + 1\n",
    "        output_shape[3] = self.filters\n",
    "\n",
    "        return tuple(output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_31 (InputLayer)       [(None, 481, 583, 3)]     0         \n",
      "                                                                 \n",
      " proposed_layer_33 (Proposed  (None, 481, 583, 16)     13460320  \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " max_pooling2d_40 (MaxPoolin  (None, 240, 291, 16)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " proposed_layer_34 (Proposed  (None, 240, 291, 12)     13409292  \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " max_pooling2d_41 (MaxPoolin  (None, 120, 145, 12)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " proposed_layer_35 (Proposed  (None, 120, 145, 8)      1670408   \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPoolin  (None, 60, 72, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 34560)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 540)               18662940  \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 2)                 1082      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,204,042\n",
      "Trainable params: 47,204,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "n_classes = 2\n",
    "\n",
    "# Input\n",
    "inputs = layers.Input(input_shape)\n",
    "\n",
    "# Block 1\n",
    "x0 = ProposedLayer(16, receptive_field_size=(3, 3), activation='relu')(inputs)\n",
    "x1 = layers.MaxPooling2D(pool_size=(2, 2))(x0)\n",
    "# Block 2\n",
    "x2 = ProposedLayer(12, receptive_field_size=(3, 3), activation='relu')(x1)\n",
    "x3 = layers.MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "# Block 3\n",
    "x4 = ProposedLayer(8, receptive_field_size=(3, 3), activation='relu')(x3)\n",
    "x5 = layers.MaxPooling2D(pool_size=(2, 2))(x4)\n",
    "\n",
    "# Flatten feature map - embedding size will become 34560\n",
    "x6 = layers.Flatten()(x5)\n",
    "\n",
    "# Dense layer for classification\n",
    "# Start with units in dense layer = embedding_size / 64\n",
    "x7 = layers.Dense(540, activation='relu')(x6)\n",
    "# Output\n",
    "outputs = layers.Dense(n_classes, activation='softmax')(x7)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
